# 基础:
## NLP任务:
### 分词:
不同的切割方式会带来不同的语意输出,例如

```
输入：雍和宫的荷花开的很好。

正确切割：雍和宫 | 的 | 荷花 | 开 | 的 | 很 | 好 | 。

错误切割 1：雍 | 和 | 宫的 | 荷花 | 开的 | 很好 | 。 （地名被拆散） 

错误切割 2：雍和 | 宫 | 的荷 | 花开 | 的很 | 好。 （词汇边界混乱）
```

对于以后词性标注,实体识别,句法分析等任务至关重要

### 词性标注:

给每个词分配一个词性,例如名词,动词,形容词等

```
假设我们有一个英文句子：She is playing the guitar in the park.

词性标注的结果如下：

- She (代词，Pronoun，PRP)
- is (动词，Verb，VBZ)
- playing (动词的现在分词，Verb，VBG)
- the (限定词，Determiner，DT)
- guitar (名词，Noun，NN)
- in (介词，Preposition，IN)
- the (限定词，Determiner，DT)
- park (名词，Noun，NN)
- . (标点，Punctuation，.)
```


通常依赖于机器学习模型,例如如隐马尔可夫模型（Hidden Markov Model，HMM）、条件随机场（Conditional Random Field，CRF）或者基于深度学习的循环神经网络 RNN 和长短时记忆网络 LSTM 等。这些模型通过学习大量的标注数据来预测新句子中每个单词的词性。

### 文本分类:
类似猫狗识别,mnist等,但是输入是文本,输出大概率是数字或者文本

```
文本：“NBA季后赛将于下周开始，湖人和勇士将在首轮对决。” 

类别：“体育” 

文本：“美国总统宣布将提高关税，引发国际贸易争端。” 

类别：“政治” 

文本：“苹果公司发布了新款 Macbook，配备了最新的m3芯片。” 

类别：“科技”
```

这一类任务大概率是监督学习,所以需要选择合适的特征表示和分类算法,以及高质量的训练数据

### 实体识别:
NER,自动识别文本中具有特定意义的实体,并将他们分类成预定意义的类别,任务,地点,组织等

```
输入：李雷和韩梅梅是北京市海淀区的居民，他们计划在2024年4月7日去上海旅行。 

输出：[("李雷", "人名"), ("韩梅梅", "人名"), ("北京市海淀区", "地名"), ("2024年4月7日", "日期"), ("上海", "地名")]
```

### 文本摘要:

主要可以分为抽取式摘要和生成式摘要:

抽取式摘要直接冲文本中选取关键句子或短语组成摘要,准确性较高,但会牺牲流畅度

生成式摘要不仅要选择文本片段,还需要对这些片段进行重新组合和该歇,通常需要更为复杂的模型,例如SEQ2SEQ

假设我们有

```
2021年5月22日，国家航天局宣布，我国自主研发的火星探测器“天问一号”成功在火星表面着陆。此次任务的成功，标志着我国在深空探测领域迈出了重要一步。“天问一号”搭载了多种科学仪器，将在火星表面进行为期90个火星日的科学探测工作，旨在研究火星地质结构、气候条件以及寻找生命存在的可能性。
```

抽取式摘要会返回类似

```
我国自主研发的火星探测器“天问一号”成功在火星表面着陆，标志着我国在深空探测领域迈出了重要一步。
```

生成式摘要:
```
“天问一号”探测器成功实现火星着陆，代表我国在宇宙探索中取得重大进展。
```

### 机器翻译:
多使用Seq2Seq, Transformer模型等


### 自动问答:
大致可以分成
- 检索式问答
- 知识库问答
- 社区问答
都需要找对应的资料,只是找的资料类别不一样

## 发展历程:
### 词向量:
将文本转换到高位空间的向量实现文本的数学化表示, 每个维度代表一个特征项,例如字词或短语等

向量中的每一个值表示该特征值在文本中的权重,通过特定的计算公式(词频TF,逆文档频率TF-IDF等)来确定,反应了特征项在文本中的重要程度

向量空间模型可以实现多种任务,通过特征值计算和SVD等可以优化文本向量表示

但是向量空间模型也存在数据洗属性和维数灾难问题等,因为多数元素值为0,冰鞋维度很高,同时忽略了次序和上下文信息等

```
# "雍和宫的荷花很美"
# 词汇表大小：16384，句子包含词汇：["雍和宫", "的", "荷花", "很", "美"] = 5个词

vector = [0, 0, ..., 1, 0, ..., 1, 0, ..., 1, 0, ..., 1, 0, ..., 1, 0, ...]
#                    ↑          ↑          ↑          ↑          ↑
#      16384维中只有5个位置为1，其余16379个位置为0
# 实际有效维度：仅5维（非零维度）
# 稀疏率：(16384-5)/16384 ≈ 99.97%
```

### N-gram模型
通过马尔科夫链的方式,通过前n-1个词估计下一个词的概率,但是n较大的时候容易出现数据洗属性问题,导致模型无法有效学习,泛化能力下降等

### Word2Vec:

学习词与词之间的上下文关系来生成词的密集向量表示,从而是的语意相似或相关的词在向量空间中距离较近